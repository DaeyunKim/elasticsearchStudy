# Analyze



분석기 : 텍스트의 토큰을 나누어 결과를리턴, 텍스트 분석 프로세스 이다.



```json
GET _analyze
{
  "analyzer": "standard",
  "text" : "this is a test"
}
```

명시화된 인덱스 없이 많은 분석기중 하나를 사용 할 수 있다.

결과 : 

```json
{
  "tokens" : [
    {
      "token" : "this",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "is",
      "start_offset" : 5,
      "end_offset" : 7,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "a",
      "start_offset" : 8,
      "end_offset" : 9,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "test",
      "start_offset" : 10,
      "end_offset" : 14,
      "type" : "<ALPHANUM>",
      "position" : 3
    }
  ]
}

```



```json
GET _analyze
{
  "analyzer": "standard",
  "text" : ["this is a test", "the second text"]
}
```

텍스트 배열을 가지고 분석기를 돌리면 어떻게 offset을 나타내는지 알아보면 다중  값 필드로 인식하고 분석한다.

아래와 같이 , 첫번째 문장의 뒤에 두번째 문장이 연결되어 offset을 나타낸다.

```json
{
  "tokens" : [
    {
      "token" : "this",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "is",
      "start_offset" : 5,
      "end_offset" : 7,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "a",
      "start_offset" : 8,
      "end_offset" : 9,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "test",
      "start_offset" : 10,
      "end_offset" : 14,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "the",
      "start_offset" : 15,
      "end_offset" : 18,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "second",
      "start_offset" : 19,
      "end_offset" : 25,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "text",
      "start_offset" : 26,
      "end_offset" : 30,
      "type" : "<ALPHANUM>",
      "position" : 6
    }
  ]
}

```



토큰 필터의 파라미터를 lowercase로 주고, 필터를 소문자로 설정하면, 전부다 하나의 토큰으로 출력되고 내용은 소문자로  출력된다.



```json
GET _analyze
{
    "tokenizer": "keyword",
    "filter" : ["lowercase"],
    "text" : "this is a test"
}
```



아래의 char_filter는 html 태그의 스크립을 제거후 결과를 나타낸다.

```json
GET _analyze
{
  "tokenizer" : "keyword",
  "filter" : ["lowercase"],
  "char_filter" : ["html_strip"],
  "text" : "this is a <b>test</b>"
}
```

결과 : 

```json
{
  "tokens" : [
    {
      "token" : "this is a test",
      "start_offset" : 0,
      "end_offset" : 21,
      "type" : "word",
      "position" : 0
    }
  ]
}
```



**custom 토크나이져**

아래의 내용을 해보니 우선 text를 lowercase로 변경하고 stop 불용어를 제거하는듯하다. 

```json
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "lowercase",
    {
      "type": "stop",
      "stopwords": [
        "a",
        "is",
        "this"
      ]
    }
  ],
  "text": "This is a test"
}
```



결과는 

```json
{
  "tokens" : [
    {
      "token" : "test",
      "start_offset" : 10,
      "end_offset" : 14,
      "type" : "word",
      "position" : 3
    }
  ]
}
```



분석기는 맵핑 기반으로도 활용 할 수 있다.

```json
GET analyze_indiex/_analyze
{
    "field": "obj1.field1","text": "this is a test"
}
```

위의 분석은 기본 매핑분석기에 대해 매핑에 고성된 분석기를 기반으로 수행하게 된다.



index normalizer과 연관된 노말 라이저가 있는 키워드 필드에 A를 제공 할 수 있다.



Tokenizer filter

* whitespace :  공백을 가지고 토큰을 나눈다,
* Lowercase : 모두 소문자로 토큰을 나눈다.
* Snowball : jumped 라는 것에서 ed, 과거형, 진행형 등 의 것을 제거한다.
* unique : 중복되는 것을 제거함

Nomalizer 참고 자료

https://www.elastic.co/guide/en/elasticsearch/reference/current/normalizer.html

Tokenizer와 Token Filter 블로그정리 

http://asuraiv.blogspot.com/2015/06/elasticsearch-tokenizer-token-filter.html













https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html